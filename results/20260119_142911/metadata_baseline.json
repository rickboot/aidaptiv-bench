{
  "timestamp": "2026-01-19T14:41:23.916751",
  "git_commit": "80b67f8bb97df09f23bf9c3e968a1d5229669a69",
  "platform": {
    "system": "Darwin",
    "release": "24.3.0",
    "machine": "arm64",
    "cpu_cores": 10,
    "ram_total_gb": 16.0,
    "ram_limit_gb": 12.0,
    "vram_limit_gb": 11.0
  },
  "test_config": {
    "scenario_name": "Llama-8B_1K-128K_double",
    "model": "llama3.1:8b",
    "concurrency": 1,
    "runs_per_context": 1,
    "step_mode": "geometric",
    "context_lengths": [
      1024,
      2048,
      4096,
      8192,
      16384,
      32768,
      65536,
      131072
    ],
    "scenario": "synthetic",
    "max_tokens_output": 50,
    "temperature": 0.0,
    "top_p": 0.9,
    "seed": 42
  },
  "config": {
    "platform": {
      "name": "local_mac_16gb",
      "ram_gb": 12.0,
      "vram_gb": 11.0
    },
    "runtime": {
      "backend": "vllm",
      "endpoint": "http://localhost:11434/v1/completions",
      "model_name": "llama3.1:8b"
    },
    "aidaptiv": {
      "toggle_method": "manual",
      "storage_device": "nvme0n1"
    },
    "test": {
      "context_lengths": [
        1024,
        2048,
        4096,
        8192,
        16384,
        32768,
        65536,
        131072
      ],
      "concurrency": 1,
      "runs_per_context": 1,
      "warmup_runs": 1,
      "max_tokens_output": 50,
      "temperature": 0.0,
      "top_p": 0.9,
      "seed": 42,
      "timeout_seconds": 300,
      "scenario_name": "Llama-8B_1K-128K_double",
      "step_mode": "geometric",
      "ram_limit": 16.0,
      "swap_limit": 32.0
    },
    "telemetry": {
      "sample_interval_sec": 0.2,
      "collect_disk_io": true,
      "output_file": "metrics.csv"
    }
  },
  "runtime_version": "Unknown"
}