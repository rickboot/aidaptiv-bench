platform:
  name: local_mac_16gb
  ram_gb: 16
  vram_gb: 24
runtime:
  backend: vllm
  endpoint: http://localhost:11434/v1/completions
  model_name: llama3.1:8b
aidaptiv:
  toggle_method: manual
  storage_device: nvme0n1
test:
  context_lengths:
  - 1024
  - 2048
  - 3072
  - 4096
  runs_per_context: 2
  warmup_runs: 1
  max_tokens_output: 50
  temperature: 0.0
  timeout_seconds: 300
telemetry:
  sample_interval_sec: 1.0
  collect_disk_io: true
  output_file: metrics.csv
